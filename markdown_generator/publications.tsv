pub_date	title	venue	excerpt	citation	url_slug	paper_url
2013-12-13	Narrative Fragment Creation: An Approach for Learning Narrative Knowledge	Advances in Cognitive Systems	Storytelling is an integral part of the human experience, and understanding how stories - or narratives - are generated can offer insight into their importance. Current research focuses on the introduction of narrative knowledge into the generation process in order to facilitate the creation of qualitatively good narratives. Narrative generation and narrative knowledge, however, are two sides of the same coin; as knowledge about narrative events is necessary for generation, so is the ability to generate narratives indispensable to understanding the events therein. We propose the narrative fragment - a construct intended to capture narrative knowledge - and a method for automatically creating these fragments with narrative generation through partial order planning and analysis through n-gram modeling. The generated plans establish causal and temporal relationships, and by modeling those relationships and creating fragments, our system learns narrative knowledge.	C. Cervantes & W. Fu. (2013) Narrative Fragment Creation: An Approach for Learning Narrative Knowledge. Conference on Advances in Cognitive Systems (ACS)	cervantes_narrative	https://cmcervantes.github.io/files/cervantes_2013_narrative.pdf
2015-12-13	Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models	International Conference on Computer Vision	The Flickr30k dataset has become a standard benchmark for sentence-based image description.  This paper presents Flickr30k Entities, which augments the 158k captions from Flickr30k with 244k coreference chains linking mentions of the same entities in images, as well as 276k manually annotated bounding boxes corresponding to each entity. Such annotation is essential for continued progress in automatic image description and grounded language understanding.  We present experiments demonstrating the usefulness of our annotations for text-to-image reference resolution, or the task of localizing textual entity mentions in an image, and for bidirectional image-sentence retrieval. These experiments confirm that we can further improve the accuracy of state-of-the-art retrieval methods by training with explicit region-to-phrase correspondence, but at the same time, they show that accurately inferring this correspondence given an image and caption remains really challenging. Our dataset is available for download at http://web.engr.illinois.edu/ Ìƒbplumme2/Flickr30kEntities/.	B. Plummer, L. Wang, C. Cervantes, J. Caicedo, J. Hockenmaier, & S. Lazebnik. (2015) Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models. International Conference on Computer Vision (ICCV)	plummer_flickr30kEntities	https://cmcervantes.github.io/files/plummer_2015_flickr30kEntities.pdf
2017-01-01	Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models	International Journal of Computer Vision 		B. Plummer, L. Wang, C. Cervantes, J. Caicedo, J. Hockenmaier, & S. Lazebnik. (2017) Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models. International Journal of Computer Vision (IJCV)	plummer_flickr30kEntities	https://cmcervantes.github.io/files/plummer_2017_flickr30kEntities.pdf
2017-10-22	Phrase Localization and Visual Relationship Detection with Comprehensive Image-Language Cues	International Conference on Computer Vision		B. Plummer, A. Mallya, C. Cervantes, J. Hockenmaier, & S. Lazebnik. (2017) Phrase Localization and Visual Relationship Detection with Comprehensive Image-Language Cues. International Conference on Computer Vision (ICCV)	plummer_phrase	https://cmcervantes.github.io/files/plummer_2017_phrase.pdf


