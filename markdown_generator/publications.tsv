pub_date	title	venue	excerpt	citation	url_slug	paper_url
12/13/13	Narrative Fragment Creation: An Approach for Learning Narrative Knowledge	Advances in Cognitive Systems	"Storytelling is an integral part of the human experience, and understanding how stories - or narratives - are generated can offer insight into their importance. Current research focuses on the introduction of narrative knowledge into the generation process in order to facilitate the creation of qualitatively good narratives. Narrative generation and narrative knowledge, however, are two sides of the same coin; as knowledge about narrative events is necessary for generation, so is the ability to generate narratives indispensable to understanding the events therein. We propose the narrative fragment - a construct intended to capture narrative knowledge - and a method for automatically creating these fragments with narrative generation through partial order planning and analysis through n-gram modeling. The generated plans establish causal and temporal relationships, and by modeling those relationships and creating fragments, our system learns narrative knowledge."	C. Cervantes & W. Fu. (2013) Narrative Fragment Creation: An Approach for Learning Narrative Knowledge. Conference on Advances in Cognitive Systems (ACS)	cervantes_narrative	https://cmcervantes.github.io/files/cervantes_2013_narrative.pdf
12/13/15	Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models	International Conference on Computer Vision	"The Flickr30k dataset has become a standard benchmark for sentence-based image description.  This paper presents Flickr30k Entities, which augments the 158k captions from Flickr30k with 244k coreference chains linking mentions of the same entities in images, as well as 276k manually annotated bounding boxes corresponding to each entity. Such annotation is essential for continued progress in automatic image description and grounded language understanding.  We present experiments demonstrating the usefulness of our annotations for text-to-image reference resolution, or the task of localizing textual entity mentions in an image, and for bidirectional image-sentence retrieval. These experiments confirm that we can further improve the accuracy of state-of-the-art retrieval methods by training with explicit region-to-phrase correspondence, but at the same time, they show that accurately inferring this correspondence given an image and caption remains really challenging. Our dataset is available for download at http://web.engr.illinois.edu/ Ìƒbplumme2/Flickr30kEntities/."	"B. Plummer, L. Wang, C. Cervantes, J. Caicedo, J. Hockenmaier, & S. Lazebnik. (2015) Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models. International Conference on Computer Vision (ICCV)"	plummer_flickr30kEntities	https://cmcervantes.github.io/files/plummer_2015_flickr30kEntities.pdf
1/1/17	Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models	International Journal of Computer Vision	"The Flickr30k dataset has become a standard benchmark for sentence-based image description. This paper presents Flickr30k Entities, which augments the 158k captions from Flickr30k with 244k coreference chains, linking mentions of the same entities across different captions for the same image, and associating them with 276k manually annotated bounding boxes. Such annotations are essential for continued progress in automatic image description and grounded language understanding. They enable us to define a new benchmark for localization of textual entity mentions in an image. We present a strong baseline for this task that combines an image-text embedding, detectors for common objects, a color classifier, and a bias towards selecting larger objects. While our baseline rivals in accuracy more complex state-of-the-art models, we show that its gains cannot be easily parlayed into improvements on such tasks as image-sentence retrieval, thus underlining the limitations of current methods and the need for further research."	"B. Plummer, L. Wang, C. Cervantes, J. Caicedo, J. Hockenmaier, & S. Lazebnik. (2017) Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models. International Journal of Computer Vision (IJCV)"	plummer_flickr30kEntities	https://cmcervantes.github.io/files/plummer_2017_flickr30kEntities.pdf
10/22/17	Phrase Localization and Visual Relationship Detection with Comprehensive Image-Language Cues	International Conference on Computer Vision	"This  paper  presents  a  framework  for  localization  or grounding  of  phrases  in  images  using a  large  collection of  linguistic  and  visual  cues.   We  model  the  appearance, size, and position of entity bounding boxes, adjectives that contain attribute information, and spatial relationships between pairs of entities connected by verbs or prepositions. Special attention is given to relationships between people and clothing or body part mentions, as they are useful for distinguishing individuals.  We automatically learn weights for combining these cues and at test time, perform joint inference over all phrases in a caption.  The resulting system produces state of the art performance on phrase localization on the Flickr30k Entities dataset and visual relationship detection on the Stanford VRD dataset."	"B. Plummer, A. Mallya, C. Cervantes, J. Hockenmaier, & S. Lazebnik. (2017) Phrase Localization and Visual Relationship Detection with Comprehensive Image-Language Cues. International Conference on Computer Vision (ICCV)"	plummer_phrase	https://cmcervantes.github.io/files/plummer_2017_phrase.pdf
02/01/18	Entity-Based Scene Understanding for Flickr30k and MSCOCO		"We define entity-based scene understanding as the task of identifying the entities in a visual scene from multiple descriptions by a) identifying coreference and subset relations between entity mentions, and b) grounding entity mentions to image regions. We apply our models to two datasets (Flickr30K Entities v2 and MSCOCO) and show that grounding can benefit significantly from relation prediction in both cases. "	"C. Cervantes, B. Plummer, S. Lazebnik, & J. Hockenmaier. (2018) Entity-Based Scene Understanding for Flickr30k and MSCOCO. Under Review / Available Upon Request"	cervantes_entity	
